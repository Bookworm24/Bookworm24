import tensorflow as tf

x1 = tf.constant([[5.8, 4.0, 1.2, 0.2]])  # 5.8,4.0,1.2,0.2（0）
w1 = tf.constant([[-0.8, -0.34, -1.4],
                  [0.6, 1.3, 0.25],
                  [0.5, 1.45, 0.9],
                  [0.65, 0.7, -1.2]])
b1 = tf.constant([2.52, -3.1, 5.62])
y = tf.matmul(x1, w1) + b1#用于进行矩阵乘法运算。它接受两个张量作为输入，并返回相应的乘积张量。
print("x1.shape:", x1.shape)
print("w1.shape:", w1.shape)
print("b1.shape:", b1.shape)
print("y.shape:", y.shape)
print("y:", y)

#####以下代码可将输出结果y转化为概率值#####
y_dim = tf.squeeze(y)  # 去掉y中纬度1（观察y_dim与 y 效果对比）#维度一个[]代表一行
y_pro = tf.nn.softmax(y_dim)  # 使y_dim符合概率分布，输出为概率值了
"""

通常用于多类别分类问题中。softmax 函数将输入向量转换为概率分布，使得所有元素的值都在 0 到 1 之间，并且所有元素的和为 1。

"""
print("y_dim:", y_dim)
print("y_pro:", y_pro)

#请观察打印出的shape

"""
激活函数在神经网络中扮演着非常重要的角色，它能够引入非线性性质，增加网络的表达能力，并且在模型的训练中起到关键的作用。以下是激活函数的几个主要作用：

1. 引入非线性：激活函数能够将输入的线性组合转换为非线性的输出。神经网络的层次结构以及激活函数的非线性特性使其能够对复杂的数据进行建模和学习，提高模型的表达能力。

2. 解决线性不可分问题：许多实际问题中的数据是非线性可分的，只有使用非线性激活函数才能有效地拟合和分类这些数据。通过激活函数的非线性变换，神经网络可以捕捉到更加复杂的模式和关系。

3. 梯度传播：激活函数的选择对于梯度的传播和反向传播过程至关重要。合适的激活函数可以避免梯度消失或梯度爆炸的问题，使得网络的训练更加稳定和高效。

4. 压缩输入范围：某些激活函数具有将输入值限制在特定范围内的特性，如 sigmoid 函数将输入压缩到 (0, 1) 之间，tanh 函数将输入压缩到 (-1, 1) 之间。这样的范围限制有助于正则化和控制网络的输出。

常见的激活函数包括 sigmoid 函数、tanh 函数、ReLU 函数等。根据具体的问题和网络结构，选择合适的激活函数可以提高神经网络的性能和表达能力。

需要注意的是，不同的激活函数适用于不同的场景，选择合适的激活函数是神经网络设计中的一个重要决策。同时，激活函数的选择也需要考虑到梯度消失和爆炸、计算效率等方面的因素。
"""